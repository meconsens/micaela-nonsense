<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RESEARCH</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <nav>
            <div class="nav-left">
                <a class="nav-item" href="index.html">M.E.C.</a>
            </div>
            <div class="nav-right">
                <a class="nav-item" href="research.html">RESEARCH</a>
                <a class="nav-item" href="projects.html">PROJECTS</a>
                <a class="nav-item" href="cv.html">CV</a>
            </div>
        </nav>
    </header>    
    <main>
        <div class="content-box">
            <div class="content">
            <section class="research-interests">
                <h1>Research Interests</h1>
                <p> In general, I'm interested in machine learning as a tool to uncover novel signals in biology. I am equally interested in the design of these novel tools, and their exploration.
                    Some of my work involves exploring the interpretability of genome language models to learn what signals are uncovered in un-supervised pre-training. Based on these findings, I'm also interested in designing biologically-motivated and downstream-task aligned unsupervised pre-training regimes to achieve more robust discovery of underlying biological signal.</p>
            </section>
            <section class="research-papers">
                <h1>Papers</h1>
                <ul>
                    <li>
                        <h3>2025 - Transformers and genome language models </h3>
                        <p><a href="https://www.nature.com/articles/s42256-025-01007-9" target="_blank">Nature Machine Intelligence</a></p>
                        <p> Large language models based on the transformer deep learning architecture have revolutionized natural language processing. Motivated by the analogy between human language and the genome’s biological code, researchers have begun to develop genome language models (gLMs) based on transformers and related architectures. This Review explores the use of transformers and language models in genomics. We survey open questions in genomics amenable to the use of gLMs, and motivate the use of gLMs and the transformer architecture for these problems. We discuss the potential of gLMs for modelling the genome using unsupervised pretraining tasks, specifically focusing on the power of zero- and few-shot learning. We explore the strengths and limitations of the transformer architecture, as well as the strengths and limitations of current gLMs more broadly. Additionally, we contemplate the future of genomic modelling beyond the transformer architecture, based on current trends in research. This Review serves as a guide for computational biologists and computer scientists interested in transformers and language models for genomic data. </p>
                    </li>
                    <li>
                        <h3>2023 - Transforming Genomic Interpretability: A DNABERT Case Study </h3>
                        <p><a href="https://icml-compbio.github.io/2023/papers/WCBICML2023_paper118.pdf" target="_blank">ICML 2023 Comp Bio Workshop Paper </a></p>
                        <p> While deep learning algorithms, particularly transformers, have recently shown significant promise in making predictions from biological sequences, their interpretability in the context of biology has not been deeply explored. This paper focuses on the recently proposed DNABERT model and explores interpreting it’s decisions using modified Layer-wise Relevance Propagation (LRP) methods to determine what the model is learning. This score is then compared to several other interpretability methods commonly applied to transformers, including the attention-score based method proposed by the DNABERT authors. Results of mutagenesis experiments targeting regions identified by different methods show the modified LRP interpretability scores can outperform others at 20 mutations, and also show attention cannot reliably outperform random scores.</p>
                    </li>
                    <li>
                        <h3>2022 - Bulk and single-nucleus transcriptomics highlight intra-telencephalic and somatostatin neurons in Alzheimer’s disease</h3>
                        <p><a href="https://www.frontiersin.org/journals/molecular-neuroscience/articles/10.3389/fnmol.2022.903175/full" target="_blank">Frontiers in Molecular Neuroscience </a></p>
                        <p> Cortical neuron loss is a pathological hallmark of late-onset Alzheimer’s disease (AD). However, it remains unclear which neuronal subtypes beyond broad excitatory and inhibitory classes are most vulnerable. Here, we analyzed cell subtype proportion differences in AD compared to non-AD controls using 1037 post-mortem brain samples from six neocortical regions. We identified the strongest associations of AD with fewer somatostatin (SST) inhibitory neurons and intra-telencephalic (IT) excitatory neurons. Replication in three AD case-control single-nucleus RNAseq datasets most strongly supported the bulk tissue association of fewer SST neurons in AD. In depth analyses of cell type proportions with specific AD-related neuropathological and cognitive phenotypes revealed fewer SST neurons with greater brain-wide post-mortem tau and beta amyloid, as well as a faster rate of antemortem cognitive decline. In contrast, greater IT neuron proportions were associated with a slower rate of cognitive decline as well as greater residual cognition–a measure of cognitive resilience–but not canonical AD neuropathology. Our findings implicate somatostatin inhibitory and intra-telencephalic excitatory neuron subclasses in the pathogenesis of AD and in cognitive resilience to AD pathology, respectively.</p>
                    </li>
                </ul>
            </section>
            <section class="talks">
                <h1>Invited Talks</h1>
                <ul>
                    <li>
                        <h3>2025 - Genome Language Models at Google Genomics</h3>
                        <p><a href="https://drive.google.com/file/d/1asCBd6nQbZArTi4A-LV-T0iNX5lgIiVo/view?ts=68adfec3&pli=1" target="_blank">Watch here</a></p>
                    </li>
                    <li>
                        <h3>2024 - Genome Language Models at Harvard</h3>
                        <p><a href="https://www.youtube.com/watch?v=mYzKb8Sox6k" target="_blank">Watch here</a></p>
                    </li>
                    <li>
                        <h3>2023 - Transforming Genomic Interpretability at T-CAIREM Trainee Rounds</h3>
                        <p><a href="https://www.youtube.com/watch?v=8VGn_3pLzNA&t=459s" target="_blank">Watch here</a></p>
                    </li>
                </ul>
            </section>
        </div>
    </div>
    </main>
</body>
</html>
